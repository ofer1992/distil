# -*- coding: utf-8 -*-
"""Copy of Threaded SMI-Based Auto Labeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M1_B-xUnVoT2edHGApyvk3XRGaXNu0-V

# SETUP

## Repo Installation
"""
"""## Imports"""
import sys
import copy
import gc
import json
import math
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import os
import pickle
import re
import sklearn
import submodlib
import sys
import time
import torch
from torch._C import device
import torch.multiprocessing as mp
import torch.optim as optim
import zipfile
from smi_autolabeler import PartitionStrategy, SMIAutoLabeler
from distil.active_learning_strategies.badge import BADGE
from distil.active_learning_strategies.entropy_sampling import EntropySampling
from distil.active_learning_strategies.least_confidence_sampling import LeastConfidenceSampling
from distil.active_learning_strategies.margin_sampling import MarginSampling
from distil.active_learning_strategies.random_sampling import RandomSampling
from distil.active_learning_strategies.strategy import Strategy
from distil.utils.models import MnistNet, ResNet18
from distil.utils.train_helper import data_train
from distil.utils.utils import LabeledToUnlabeledDataset
from torch import nn
from torch.utils.data import ConcatDataset, DataLoader, Dataset, Subset
from torchvision import datasets, transforms
from tsnecuda import TSNE
from argparse import ArgumentParser

"""
Experimental Settings
"""
parser = ArgumentParser()
# dataset config
parser.add_argument("--experiment", "-e", default="mnist", type=str, help="Experiment setting")
parser.add_argument("--alpha", "-a", default=0.9, type=float, help="ALPHA Value")
parser.add_argument("--al_strategy", default="random", type=str, help="AL strategy")
parser.add_argument("--bl", default=False, type=bool, help="use balanced loss")
parser.add_argument("--device", default=1, type=int, help="DEVICE ID")
init_args = parser.parse_args()
experiment_name = init_args.experiment
alpha = init_args.alpha
active_learning_strategy = init_args.al_strategy
balance_loss = init_args.bl
device_id = init_args.device

"""## Pre-Experiment"""
if experiment_name == 'cifar10':
    dataset_name = "CIFAR10"
    model_name = "resnet18"
    experiment_seed_budget_caps = [(3000,3000,30000),
                               (500,500,5000),
                               (1000,1000,10000),
                               (2000,2000,20000)]
    mp.set_start_method("spawn")
    per_exp_runs = 1
    args = {'islogs': False,
            'optimizer': 'sgd',
            'isverbose': True,
            'isreset': True,
            'max_accuracy': 0.99,
            'n_epoch': 300,
            'lr': 0.001,
            'device': 'cuda:'+ str(device_id),
            'batch_size': 64,
            'thread_count': 3,
            'metric': 'cosine',
            'embedding_type': 'gradients',
            'gradType': 'bias_linear'}            
elif experiment_name == 'mnist':
    dataset_name = "MNIST"
    model_name = "mnistnet"
    experiment_seed_budget_caps = [(20,30,200),
                                (20,60,200),
                                (20,90,200),
                                (40,180,400),
                                (100,150,1000),
                                (500,2000,2500),
                                (100,900,1000),
                                (20,180,200)
                                ]
    per_exp_runs = 3
    args = {'islogs': False,
            'optimizer': 'sgd',
            'isverbose': True,
            'isreset': True,
            'max_accuracy': 0.99,
            'n_epoch': 300,
            'lr': 0.001,
            'device': 'cuda:'+ str(device_id),
            'batch_size': 64,
            'thread_count': 3,
            'metric': 'cosine',
            'embedding_type': 'gradients',
            'gradType': 'bias_linear'}
elif experiment_name == 'cifar100':
    dataset_name = "CIFAR100"
    model_name = "resnet18"
    experiment_seed_budget_caps = [(5000,5000,25000),
                                (10000,7500,40000)]
    per_exp_runs = 3
    args = {'islogs': False,
            'optimizer': 'sgd',
            'isverbose': True,
            'isreset': True,
            'max_accuracy': 0.99,
            'n_epoch': 300,
            'lr': 0.001,
            'device': 'cuda:'+ str(device_id),
            'batch_size': 64,
            'metric': 'cosine',
            'embedding_type': 'gradients',
            'gradType': 'bias_linear'}

"""## Saving Parameters"""

mount_point_directory = "results/" + dataset_name
google_drive_directory = "results/" + dataset_name
base_save_directory = "results/" + dataset_name

auto_label_no_hil_save_directory = os.path.join(base_save_directory, "auto_label_no_hil")
auto_label_no_hil_loss_balanced_save_directory = os.path.join(base_save_directory, "auto_label_no_hil_loss_balanced")
auto_label_hil_save_directory = os.path.join(base_save_directory, "auto_label_hil")

google_drive_directory = "results/" + dataset_name + "/check/"
checkpoint_directory = os.path.join(mount_point_directory, google_drive_directory)

google_drive_directory = "results/" + dataset_name+ "/model/"
model_directory = os.path.join(mount_point_directory, google_drive_directory)

dataset_root_directory = "data/"

os.makedirs(auto_label_no_hil_save_directory, exist_ok=True)
os.makedirs(auto_label_hil_save_directory, exist_ok=True)
os.makedirs(checkpoint_directory, exist_ok=True)
os.makedirs(model_directory, exist_ok=True)

"""
# EXPERIMENTS

## Definitions

### Checkpointing
"""


class Checkpoint:

    def __init__(self, exp_dict=None, idx_bit_vector=None, labels=None, state_dict=None, experiment_name=None, path=None):

        # If a path is supplied, load a checkpoint from there.
        if path is not None:

            if experiment_name is not None:
                self.load_checkpoint(path, experiment_name)
            else:
                raise ValueError("Checkpoint contains None value for experiment_name")

            return

        if exp_dict is None:
            raise ValueError("Checkpoint contains None value for acc_list")

        if idx_bit_vector is None:
            raise ValueError("Checkpoint contains None value for idx_bit_vector")

        if state_dict is None:
            raise ValueError("Checkpoint contains None value for state_dict")

        if labels is None:
            raise ValueError("Checkpoint contains None value for labels")

        if experiment_name is None:
            raise ValueError("Checkpoint contains None value for experiment_name")

        self.exp_dict = exp_dict
        self.idx_bit_vector = idx_bit_vector
        self.labels = labels
        self.state_dict = state_dict
        self.experiment_name = experiment_name

    def __eq__(self, other):

        # Check if the accuracy lists are equal
        acc_lists_equal = self.exp_dict == other.exp_dict

        # Check if the indices are equal
        indices_equal = self.idx_bit_vector == other.idx_bit_vector

        # Check if the labels are equal
        labels_equal = self.labels == other.labels

        # Check if the experiment names are equal
        experiment_names_equal = self.experiment_name == other.experiment_name

        return acc_lists_equal and indices_equal and labels_equal and experiment_names_equal

    def save_checkpoint(self, path):

        # Get current time to use in file timestamp
        timestamp = time.time_ns()

        # Create the path supplied
        os.makedirs(path, exist_ok=True)

        # Name saved files using timestamp to add recency information
        save_path = os.path.join(path, F"c{timestamp}1")

        # Write this checkpoint to the save location
        with open(save_path, 'wb') as save_file:
            pickle.dump(self, save_file)

    def load_checkpoint(self, path, experiment_name):

        # Obtain a list of all files present at the path
        timestamp_save_no = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]

        # If there are no such files, set values to None and return
        if len(timestamp_save_no) == 0:
            self.exp_dict = None
            self.idx_bit_vector = None
            self.labels = None
            self.state_dict = None
            return

        # Sort the list of strings to get the most recent
        timestamp_save_no.sort(reverse=True)

        while len(timestamp_save_no) >= 1:

            load_file = timestamp_save_no.pop(0)

            # Form the paths to the files
            load_path = os.path.join(path, load_file)

            # Load the checkpoint
            with open(load_path, 'rb') as load_file:
                checkpoint = pickle.load(load_file)

            if checkpoint.experiment_name != experiment_name:
                continue

            # This checkpoint will suffice. Populate this checkpoint's fields 
            # with the selected checkpoint's fields.
            self.exp_dict = checkpoint.exp_dict
            self.idx_bit_vector = checkpoint.idx_bit_vector
            self.labels = checkpoint.labels
            self.state_dict = checkpoint.state_dict
            return

        # Instantiate None values in acc_list, indices, and model
        self.exp_dict = None
        self.idx_bit_vector = None
        self.labels = None
        self.state_dict = None

    def get_saved_values(self):

        return (self.exp_dict, self.idx_bit_vector, self.labels, self.state_dict)

def delete_checkpoints(checkpoint_directory, experiment_name):

    # Iteratively go through each checkpoint, deleting those whose experiment name matches.
    timestamp_save_no = [f for f in os.listdir(checkpoint_directory) if os.path.isfile(os.path.join(checkpoint_directory, f))]

    for file in timestamp_save_no:

        delete_file = False

        # Get file location
        file_path = os.path.join(checkpoint_directory, file)

        if not os.path.exists(file_path):
            continue

        # Unpickle the checkpoint and see if its experiment name matches
        with open(file_path, "rb") as load_file:

            checkpoint_copy = pickle.load(load_file)
            delete_file = checkpoint_copy.experiment_name == experiment_name

        # Delete this file only if the experiment name matched
        if delete_file:
            os.remove(file_path)

"""### Evaluation Utilities"""

def get_label_counts(dataset, nclasses, batch_size=64):

    label_counts = [0 for x in range(nclasses)]
    dataloader = DataLoader(dataset, shuffle=False, batch_size=batch_size)

    with torch.no_grad():
        for batch_idx, (data, labels) in enumerate(dataloader):
            for cls in range(nclasses):
                count = len(torch.where(labels==cls)[0])
                label_counts[cls] += count

    return label_counts

def get_labels(dataset, batch_size=64):

    dataloader = DataLoader(dataset, shuffle=False, batch_size=batch_size)
    
    all_labels = []

    with torch.no_grad():
        for batch_idx, (data, labels) in enumerate(dataloader):
            all_labels.extend(labels)

    return torch.tensor(all_labels)

"""### Replace Label Dataset"""

class ReplaceLabelDataset(Dataset):

    def __init__(self, labeled_dataset, new_label_sequence):
        self.labeled_dataset = labeled_dataset
        self.new_label_sequence = new_label_sequence

    def __getitem__(self, index):
        data, old_index = self.labeled_dataset[index]
        new_index = self.new_label_sequence[index]
        return data, new_index

    def __len__(self):
        return len(self.new_label_sequence)

"""### Selection Utilities"""

def get_class_subset(dataset, class_to_retrieve, batch_size=64):

    dataloader = DataLoader(dataset, shuffle=False, batch_size=batch_size)
    subset_idxs = []
    eval_idxs = 0

    with torch.no_grad():
        for batch_idx, (data, labels) in enumerate(dataloader):

            matching_class_batch_idxs = torch.where(labels==class_to_retrieve)[0]
            matching_class_batch_idxs = matching_class_batch_idxs + eval_idxs
            subset_idxs.extend(matching_class_batch_idxs)
            eval_idxs += len(labels)

    return Subset(dataset, subset_idxs)

def label_new_points(unlabeled_dataset, to_add_idx_class_list, selection_mode):
    if selection_mode == "auto":
        new_labels_list = [label for (_,label) in to_add_idx_class_list]
        selected_idx = [index for (index,_) in to_add_idx_class_list]
        return ReplaceLabelDataset(Subset(unlabeled_dataset, selected_idx), new_labels_list)
    elif selection_mode == "hil":
        selected_idx = [index for (index,_) in to_add_idx_class_list]
        return Subset(unlabeled_dataset, selected_idx)

"""### Default Training Class"""

def init_weights(m):
    if type(m) == nn.Linear:
        torch.nn.init.xavier_uniform_(m.weight)
        m.bias.data.fill_(0.01)

class AddIndexDataset(Dataset):
    
    def __init__(self, wrapped_dataset):
        self.wrapped_dataset = wrapped_dataset
        
    def __getitem__(self, index):
        data, label = self.wrapped_dataset[index]
        return data, label, index
    
    def __len__(self):
        return len(self.wrapped_dataset)

#custom training
class data_train:

    """
    Provides a configurable training loop for AL.
    
    Parameters
    ----------
    training_dataset: torch.utils.data.Dataset
        The training dataset to use
    net: torch.nn.Module
        The model to train
    args: dict
        Additional arguments to control the training loop
        
        `batch_size` - The size of each training batch (int, optional)
        `islogs`- Whether to return training metadata (bool, optional)
        `optimizer`- The choice of optimizer. Must be one of 'sgd' or 'adam' (string, optional)
        `isverbose`- Whether to print more messages about the training (bool, optional)
        `isreset`- Whether to reset the model before training (bool, optional)
        `max_accuracy`- The training accuracy cutoff by which to stop training (float, optional)
        `min_diff_acc`- The minimum difference in accuracy to measure in the window of monitored accuracies. If all differences are less than the minimum, stop training (float, optional)
        `window_size`- The size of the window for monitoring accuracies. If all differences are less than 'min_diff_acc', then stop training (int, optional)
        `criterion`- The criterion to use for training (typing.Callable[], optional)
        `device`- The device to use for training (string, optional)
    """
    
    def __init__(self, training_dataset, net, args):

        self.training_dataset = AddIndexDataset(training_dataset)
        self.net = net
        self.args = args
        
        self.n_pool = len(training_dataset)
        
        if 'islogs' not in args:
            self.args['islogs'] = False

        if 'optimizer' not in args:
            self.args['optimizer'] = 'sgd'
        
        if 'isverbose' not in args:
            self.args['isverbose'] = False
        
        if 'isreset' not in args:
            self.args['isreset'] = True

        if 'max_accuracy' not in args:
            self.args['max_accuracy'] = 0.95

        if 'min_diff_acc' not in args: #Threshold to monitor for
            self.args['min_diff_acc'] = 0.001

        if 'window_size' not in args:  #Window for monitoring accuracies
            self.args['window_size'] = 10
            
        if 'criterion' not in args:
            self.args['criterion'] = nn.CrossEntropyLoss()
            
        if 'device' not in args:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = args['device']

    def update_index(self, idxs_lb):
        self.idxs_lb = idxs_lb

    def update_data(self, new_training_dataset):
        """
        Updates the training dataset with the provided new training dataset
        
        Parameters
        ----------
        new_training_dataset: torch.utils.data.Dataset
            The new training dataset
        """
        self.training_dataset = AddIndexDataset(new_training_dataset)

    def get_acc_on_set(self, test_dataset):
        
        """
        Calculates and returns the accuracy on the given dataset to test
        
        Parameters
        ----------
        test_dataset: torch.utils.data.Dataset
            The dataset to test
        Returns
        -------
        accFinal: float
            The fraction of data points whose predictions by the current model match their targets
        """	
        
        try:
            self.clf
        except:
            self.clf = self.net

        if test_dataset is None:
            raise ValueError("Test data not present")
        
        if 'batch_size' in self.args:
            batch_size = self.args['batch_size']
        else:
            batch_size = 1 
        
        loader_te = DataLoader(test_dataset, shuffle=False, pin_memory=True, batch_size=batch_size)
        self.clf.eval()
        accFinal = 0.

        with torch.no_grad():        
            self.clf = self.clf.to(device=self.device)
            for batch_id, (x,y) in enumerate(loader_te):     
                x, y = x.to(device=self.device), y.to(device=self.device)
                out = self.clf(x)
                accFinal += torch.sum(1.0*(torch.max(out,1)[1] == y)).item() #.data.item()

        return accFinal / len(test_dataset)

    def _train_weighted(self, epoch, loader_tr, optimizer, gradient_weights):
        self.clf.train()
        accFinal = 0.
        criterion = self.args['criterion']
        criterion.reduction = "none"

        for batch_id, (x, y, idxs) in enumerate(loader_tr):
            x, y = x.to(device=self.device), y.to(device=self.device)
            gradient_weights = gradient_weights.to(device=self.device)

            optimizer.zero_grad()
            out = self.clf(x)

            # Modify the loss function to apply weights before reducing to a mean
            loss = criterion(out, y.long())

            # Perform a dot product with the loss vector and the weight vector, then divide by batch size.
            weighted_loss = torch.dot(loss, gradient_weights[idxs])
            weighted_loss = torch.div(weighted_loss, len(idxs))

            accFinal += torch.sum(torch.eq(torch.max(out,1)[1],y)).item() #.data.item()

            # Backward now does so on the weighted loss, not the regular mean loss
            weighted_loss.backward() 

            # clamp gradients, just in case
            # for p in filter(lambda p: p.grad is not None, self.clf.parameters()): p.grad.data.clamp_(min=-.1, max=.1)

            optimizer.step()
        return accFinal / len(loader_tr.dataset), weighted_loss

    def _train(self, epoch, loader_tr, optimizer):
        self.clf.train()
        accFinal = 0.
        criterion = self.args['criterion']
        criterion.reduction = "mean"

        for batch_id, (x, y, idxs) in enumerate(loader_tr):
            x, y = x.to(device=self.device), y.to(device=self.device)

            optimizer.zero_grad()
            out = self.clf(x)
            loss = criterion(out, y.long())
            accFinal += torch.sum((torch.max(out,1)[1] == y).float()).item()
            loss.backward()

            # clamp gradients, just in case
            # for p in filter(lambda p: p.grad is not None, self.clf.parameters()): p.grad.data.clamp_(min=-.1, max=.1)

            optimizer.step()
        return accFinal / len(loader_tr.dataset), loss

    def check_saturation(self, acc_monitor):
        
        saturate = True

        for i in range(len(acc_monitor)):
            for j in range(i+1, len(acc_monitor)):
                if acc_monitor[j] - acc_monitor[i] >= self.args['min_diff_acc']:
                    saturate = False
                    break

        return saturate

    def train(self, gradient_weights=None):

        """
        Initiates the training loop.
        
        Parameters
        ----------
        gradient_weights: list, optional
            The weight of each data point's effect on the loss gradient. If none, regular training will commence. If not, weighted training will commence.
        Returns
        -------
        model: torch.nn.Module
            The trained model. Alternatively, this will also return the training logs if 'islogs' is set to true.
        """        

        print('Training..')
        def weight_reset(m):
            if hasattr(m, 'reset_parameters'):
                m.reset_parameters()

        train_logs = []
        n_epoch = self.args['n_epoch']
        
        if self.args['isreset']:
            self.clf = self.net.apply(weight_reset).to(device=self.device)
        else:
            try:
                self.clf
            except:
                self.clf = self.net.apply(weight_reset).to(device=self.device)

        if self.args['optimizer'] == 'sgd':
            optimizer = optim.SGD(self.clf.parameters(), lr = self.args['lr'], momentum=0.9, weight_decay=5e-4)
            lr_sched = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epoch)
        
        elif self.args['optimizer'] == 'adam':
            optimizer = optim.Adam(self.clf.parameters(), lr = self.args['lr'], weight_decay=0)

        
        if 'batch_size' in self.args:
            batch_size = self.args['batch_size']
        else:
            batch_size = 1

        # Set shuffle to true to encourage stochastic behavior for SGD
        loader_tr = DataLoader(self.training_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)
        epoch = 1
        accCurrent = 0
        is_saturated = False
        acc_monitor = []

        while (accCurrent < self.args['max_accuracy']) and (epoch < n_epoch) and (not is_saturated): 
            
            if gradient_weights is None:
                accCurrent, lossCurrent = self._train(epoch, loader_tr, optimizer)
            else:
                accCurrent, lossCurrent = self._train_weighted(epoch, loader_tr, optimizer, gradient_weights)
            
            acc_monitor.append(accCurrent)

            if self.args['optimizer'] == 'sgd':
                lr_sched.step()
            
            epoch += 1
            if(self.args['isverbose']):
                if epoch % 50 == 0:
                    print(str(epoch) + ' training accuracy: ' + str(accCurrent), flush=True)

            #Stop training if not converging
            if len(acc_monitor) >= self.args['window_size']:

                is_saturated = self.check_saturation(acc_monitor)
                del acc_monitor[0]

            log_string = 'Epoch:' + str(epoch) + '- training accuracy:'+str(accCurrent)+'- training loss:'+str(lossCurrent)
            train_logs.append(log_string)
            if (epoch % 50 == 0) and (accCurrent < 0.2): # resetif not converging
                self.clf = self.net.apply(weight_reset).to(device=self.device)
                
                if self.args['optimizer'] == 'sgd':

                    optimizer = optim.SGD(self.clf.parameters(), lr = self.args['lr'], momentum=0.9, weight_decay=5e-4)
                    lr_sched = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epoch)

                else:
                    optimizer = optim.Adam(self.clf.parameters(), lr = self.args['lr'], weight_decay=0)

        print('Epoch:', str(epoch), 'Training accuracy:', round(accCurrent, 3), flush=True)

        if self.args['islogs']:
            return self.clf, train_logs
        else:
            return self.clf

"""### Labeled-Autolabeled Loss-Balanced Training Class"""

def init_weights(m):
    if type(m) == nn.Linear:
        torch.nn.init.xavier_uniform_(m.weight)
        m.bias.data.fill_(0.01)

#custom training
class data_train_balanced:

    """
    Provides a configurable training loop for AL.
    
    Parameters
    ----------
    training_dataset: torch.utils.data.Dataset
        The training dataset to use
    net: torch.nn.Module
        The model to train
    args: dict
        Additional arguments to control the training loop
        
        `batch_size` - The size of each training batch (int, optional)
        `islogs`- Whether to return training metadata (bool, optional)
        `optimizer`- The choice of optimizer. Must be one of 'sgd' or 'adam' (string, optional)
        `isverbose`- Whether to print more messages about the training (bool, optional)
        `isreset`- Whether to reset the model before training (bool, optional)
        `max_accuracy`- The training accuracy cutoff by which to stop training (float, optional)
        `min_diff_acc`- The minimum difference in accuracy to measure in the window of monitored accuracies. If all differences are less than the minimum, stop training (float, optional)
        `window_size`- The size of the window for monitoring accuracies. If all differences are less than 'min_diff_acc', then stop training (int, optional)
        `criterion`- The criterion to use for training (typing.Callable[], optional)
        `device`- The device to use for training (string, optional)
    """
    
    def __init__(self, training_dataset, auto_labeled_indices, net, args):

        self.training_dataset = AddIndexDataset(training_dataset)
        self.auto_labeled_indices = [] if auto_labeled_indices is None else auto_labeled_indices
        self.net = net
        self.args = args
        
        self.n_pool = len(training_dataset)
        
        if 'islogs' not in args:
            self.args['islogs'] = False

        if 'optimizer' not in args:
            self.args['optimizer'] = 'sgd'
        
        if 'isverbose' not in args:
            self.args['isverbose'] = False
        
        if 'isreset' not in args:
            self.args['isreset'] = True

        if 'max_accuracy' not in args:
            self.args['max_accuracy'] = 0.95

        if 'min_diff_acc' not in args: #Threshold to monitor for
            self.args['min_diff_acc'] = 0.001

        if 'window_size' not in args:  #Window for monitoring accuracies
            self.args['window_size'] = 10
            
        if 'criterion' not in args:
            self.args['criterion'] = nn.CrossEntropyLoss()
            
        if 'device' not in args:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = args['device']

    def update_index(self, idxs_lb):
        self.idxs_lb = idxs_lb

    def update_data(self, new_training_dataset, new_auto_labeled_indices):
        """
        Updates the training dataset with the provided new training dataset
        
        Parameters
        ----------
        new_training_dataset: torch.utils.data.Dataset
            The new training dataset
        """

        self.training_dataset = AddIndexDataset(new_training_dataset)
        self.auto_labeled_indices = new_auto_labeled_indices

    def get_acc_on_set(self, test_dataset):
        
        """
        Calculates and returns the accuracy on the given dataset to test
        
        Parameters
        ----------
        test_dataset: torch.utils.data.Dataset
            The dataset to test
        Returns
        -------
        accFinal: float
            The fraction of data points whose predictions by the current model match their targets
        """	
        
        try:
            self.clf
        except:
            self.clf = self.net

        if test_dataset is None:
            raise ValueError("Test data not present")
        
        if 'batch_size' in self.args:
            batch_size = self.args['batch_size']
        else:
            batch_size = 1 
        
        loader_te = DataLoader(test_dataset, shuffle=False, pin_memory=True, batch_size=batch_size)
        self.clf.eval()
        accFinal = 0.

        with torch.no_grad():        
            self.clf = self.clf.to(device=self.device)
            for batch_id, (x,y) in enumerate(loader_te):     
                x, y = x.to(device=self.device), y.to(device=self.device)
                out = self.clf(x)
                accFinal += torch.sum(1.0*(torch.max(out,1)[1] == y)).item() #.data.item()

        return accFinal / len(test_dataset)

    def _train(self, epoch, loader_tr, optimizer):
        self.clf.train()
        accFinal = 0.
        criterion = self.args['criterion']
        criterion.reduction = "mean"

        for batch_id, (x, y, idxs) in enumerate(loader_tr):
            x, y = x.to(device=self.device), y.to(device=self.device)

            optimizer.zero_grad()
            out = self.clf(x)

            # Determine which points have auto-labels
            auto_labeled_idx = list(set(idxs).intersection(set(self.auto_labeled_indices)))
            true_labeled_idx = list(set(idxs) - set(auto_labeled_idx))

            if len(auto_labeled_idx) == 0:
                loss = criterion(out, y.long())
                loss.backward()
            else:

                # Separate model output into auto-labeled group and true-labeled group
                auto_out = out[auto_labeled_idx]
                true_out = out[true_labeled_idx]
                auto_y = y[auto_labeled_idx]
                true_y = y[true_labeled_idx]

                # Apply criterion to get loss of each group
                # Note: criterion.reduction is set to mean, so
                # there is no need to divide ourselves. 
                auto_loss = criterion(auto_out, auto_y.long())
                true_loss = criterion(true_out, true_y.long())

                # Combine losses and do backward
                full_loss = auto_loss + true_loss
                full_loss.backward()

            accFinal += torch.sum((torch.max(out,1)[1] == y).float()).item()
            optimizer.step()
        return accFinal / len(loader_tr.dataset), loss

    def check_saturation(self, acc_monitor):
        
        saturate = True

        for i in range(len(acc_monitor)):
            for j in range(i+1, len(acc_monitor)):
                if acc_monitor[j] - acc_monitor[i] >= self.args['min_diff_acc']:
                    saturate = False
                    break

        return saturate

    def train(self, gradient_weights=None):

        """
        Initiates the training loop.
        
        Parameters
        ----------
        gradient_weights: list, optional
            The weight of each data point's effect on the loss gradient. If none, regular training will commence. If not, weighted training will commence.
        Returns
        -------
        model: torch.nn.Module
            The trained model. Alternatively, this will also return the training logs if 'islogs' is set to true.
        """        

        print('Training..')
        def weight_reset(m):
            if hasattr(m, 'reset_parameters'):
                m.reset_parameters()

        train_logs = []
        n_epoch = self.args['n_epoch']
        
        if self.args['isreset']:
            self.clf = self.net.apply(weight_reset).to(device=self.device)
        else:
            try:
                self.clf
            except:
                self.clf = self.net.apply(weight_reset).to(device=self.device)

        if self.args['optimizer'] == 'sgd':
            optimizer = optim.SGD(self.clf.parameters(), lr = self.args['lr'], momentum=0.9, weight_decay=5e-4)
            lr_sched = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epoch)
        
        elif self.args['optimizer'] == 'adam':
            optimizer = optim.Adam(self.clf.parameters(), lr = self.args['lr'], weight_decay=0)

        
        if 'batch_size' in self.args:
            batch_size = self.args['batch_size']
        else:
            batch_size = 1

        # Set shuffle to true to encourage stochastic behavior for SGD
        loader_tr = DataLoader(self.training_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)
        epoch = 1
        accCurrent = 0
        is_saturated = False
        acc_monitor = []

        while (accCurrent < self.args['max_accuracy']) and (epoch < n_epoch) and (not is_saturated): 
            
            if gradient_weights is None:
                accCurrent, lossCurrent = self._train(epoch, loader_tr, optimizer)
            else:
                accCurrent, lossCurrent = self._train_weighted(epoch, loader_tr, optimizer, gradient_weights)
            
            acc_monitor.append(accCurrent)

            if self.args['optimizer'] == 'sgd':
                lr_sched.step()
            
            epoch += 1
            if(self.args['isverbose']):
                if epoch % 50 == 0:
                    print(str(epoch) + ' training accuracy: ' + str(accCurrent), flush=True)

            #Stop training if not converging
            if len(acc_monitor) >= self.args['window_size']:

                is_saturated = self.check_saturation(acc_monitor)
                del acc_monitor[0]

            log_string = 'Epoch:' + str(epoch) + '- training accuracy:'+str(accCurrent)+'- training loss:'+str(lossCurrent)
            train_logs.append(log_string)
            if (epoch % 50 == 0) and (accCurrent < 0.2): # resetif not converging
                self.clf = self.net.apply(weight_reset).to(device=self.device)
                
                if self.args['optimizer'] == 'sgd':

                    optimizer = optim.SGD(self.clf.parameters(), lr = self.args['lr'], momentum=0.9, weight_decay=5e-4)
                    lr_sched = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epoch)

                else:
                    optimizer = optim.Adam(self.clf.parameters(), lr = self.args['lr'], weight_decay=0)

        print('Epoch:', str(epoch), 'Training accuracy:', round(accCurrent, 3), flush=True)

        if self.args['islogs']:
            return self.clf, train_logs
        else:
            return self.clf

"""### Highest Confidence AutoLabeler"""

class ConfidenceAutoLabeler(Strategy):

    def __init__(self, labeled_dataset, unlabeled_dataset, net, nclasses, args={}): #
        
        super(ConfidenceAutoLabeler, self).__init__(labeled_dataset, unlabeled_dataset, net, nclasses, args)

    def select(self, budget):
        """
        Selects next set of points
        
        Parameters
        ----------
        budget: int
            Number of data points to select for labeling
            
        Returns
        ----------
        idxs: list
            List of selected data point indices with respect to unlabeled_dataset
        """	

        self.model.eval()       

        # Get the model's predictions
        probs = self.predict_prob(self.unlabeled_dataset)
        max_class_prob, class_predictions = torch.max(probs, dim=1)
        sorted_max_class_prob, sorted_max_class_indices = torch.sort(max_class_prob, descending=True)
        sorted_class_predictions = class_predictions[sorted_max_class_indices]

        budgets_to_use = [(budget * i) // self.target_classes for i in range(self.target_classes + 1)]
        selected_idx = []

        for sel_class in range(self.target_classes):

            class_budget = budgets_to_use[sel_class + 1] - budgets_to_use[sel_class]
            sel_class_idx = torch.where(sorted_class_predictions == sel_class)[0]
            sel_class_to_label_idx = (sorted_max_class_indices[sel_class_idx])[:class_budget]
            sel_class_to_label_prob = (sorted_max_class_prob[sel_class_idx])[:class_budget]
            sel_class_list = zip(sel_class_to_label_idx.tolist(), sel_class_to_label_prob.tolist())
            selected_idx.append(sel_class_list)

        return selected_idx

"""### Training Loop"""

def al_train_loop(full_dataset, train_lake_usage_list, test_dataset, net, n_rounds, budget, args, nclasses, alpha, beta, auto_labeling_name, active_learning_name, checkpoint_directory, experiment_name, balance_loss=False):

    # Calculate remaining portion of budget, gamma.
    #   alpha:  Percent auto-labeled
    #   beta:   Percent human-corrected suggested labels 
    #   gamma:  Percent pure AL
    gamma = 1 - alpha - beta

    # Get all labels in the full dataset as the initial assigned labels
    assigned_labels = get_labels(full_dataset, args['batch_size'])

    # Get the initial train set size
    initial_seed_size = len([i for (i,x) in enumerate(train_lake_usage_list) if x == 1])

    # Define initial experiment dictionary
    exp_dict = {'set_sizes':[],
                'test_accuracies':[],
                'budget':budget,
                'auto_labeling_function': auto_labeling_name,
                'active_learning_strategy': active_learning_name,
                'auto_assigned_selected_idx': [],
                'human_corrected_selected_idx': [],
                'active_learning_selected_idx': [],
                'auto_assigned_selection_matrices': [],
                'human_corrected_selection_matrices': [],
                'auto_selection_times': [],
                'al_selection_times': [],
                'train_times': []
                }

    # Set the initial round to 1
    initial_round = 1

    # Obtain a checkpoint if one exists
    training_checkpoint = Checkpoint(experiment_name=experiment_name, path=checkpoint_directory)
    rec_exp_dict, rec_train_lake_usage_list, rec_labels, rec_state_dict = training_checkpoint.get_saved_values()

    # Check if there are values to recover
    if rec_exp_dict is not None:

        # Restore the experiment dict
        exp_dict = rec_exp_dict

        # Restore the train-lake usage list
        train_lake_usage_list = rec_train_lake_usage_list

        # Restore the auto-assigned labels
        assigned_labels = rec_labels

        # Restore the model
        net.load_state_dict(rec_state_dict)

        # Fix the initial round
        new_train_set_size = len([i for (i,x) in enumerate(train_lake_usage_list) if x == 1])
        initial_round = (new_train_set_size - initial_seed_size) // budget + 1

    # Ensure the loaded model is moved to the right device
    net = net.to(args['device'])

    # Obtain the labeled/lake datasets
    train_indices = [i for (i,x) in enumerate(train_lake_usage_list) if x == 1]
    train_dataset = Subset(ReplaceLabelDataset(full_dataset, assigned_labels), train_indices)
    lake_indices = [i for (i,x) in enumerate(train_lake_usage_list) if x == 0]
    lake_dataset = Subset(full_dataset, lake_indices)

    # Initialize the training helper
    auto_labeled_indices = []

    if balance_loss:
        dt = data_train_balanced(train_dataset, auto_labeled_indices, net, args)
    else:
        dt = data_train(train_dataset, net, args)  

    # Get information about the initial model this is the first round
    if initial_round == 1:
        initial_test_acc = dt.get_acc_on_set(test_dataset)
        exp_dict['test_accuracies'].append(initial_test_acc)
        exp_dict['set_sizes'].append(len(train_dataset))
        print("Initial Test Accuracy:", round(initial_test_acc*100, 2), flush=True)

    # Initialize the AL strategy.
    if active_learning_name == "badge":
        strat_args = copy.deepcopy(args)
        active_learning_strategy = BADGE(train_dataset, LabeledToUnlabeledDataset(lake_dataset), net, nclasses, strat_args)
    elif active_learning_name == "entropy":
        strat_args = copy.deepcopy(args)
        active_learning_strategy = EntropySampling(train_dataset, LabeledToUnlabeledDataset(lake_dataset), net, nclasses, strat_args)
    elif active_learning_name == "least_confidence":
        strat_args = copy.deepcopy(args)
        active_learning_strategy = LeastConfidenceSampling(train_dataset, LabeledToUnlabeledDataset(lake_dataset), net, nclasses, strat_args)
    elif active_learning_name == "margin":
        strat_args = copy.deepcopy(args)
        active_learning_strategy = MarginSampling(train_dataset, LabeledToUnlabeledDataset(lake_dataset), net, nclasses, strat_args)
    elif active_learning_name == "random":
        strat_args = copy.deepcopy(args)
        active_learning_strategy = RandomSampling(train_dataset, LabeledToUnlabeledDataset(lake_dataset), net, nclasses, strat_args)
    else:
        raise ValueError("active_learning_strategy should take one of ['badge', 'entropy, 'least_confidence', 'margin', 'random']")

    # Initialize the auto-labeling strategy.
    if auto_labeling_name == "fl1mi":
        strat_args = copy.deepcopy(args)
        strat_args['optimizer'] = "LazyGreedy"
        strat_args['wrapped_strategy_class'] = SMIAutoLabeler
        strat_args['smi_function'] = 'fl1mi'
        auto_labeling_strategy = PartitionStrategy(train_dataset, LabeledToUnlabeledDataset(lake_dataset), net, nclasses, strat_args, query_dataset=None)
    elif auto_labeling_name == "fl2mi":
        strat_args = copy.deepcopy(args)
        strat_args['optimizer'] = "LazyGreedy"
        strat_args['wrapped_strategy_class'] = SMIAutoLabeler
        strat_args['smi_function'] = 'fl2mi'
        auto_labeling_strategy = PartitionStrategy(train_dataset, LabeledToUnlabeledDataset(lake_dataset), net, nclasses, strat_args, query_dataset=None)
    elif auto_labeling_name == "gcmi":
        strat_args = copy.deepcopy(args)
        strat_args['optimizer'] = "LazyGreedy"
        strat_args['wrapped_strategy_class'] = SMIAutoLabeler
        strat_args['smi_function'] = 'gcmi'
        auto_labeling_strategy = PartitionStrategy(train_dataset, LabeledToUnlabeledDataset(lake_dataset), net, nclasses, strat_args, query_dataset=None)
    elif auto_labeling_name == "logdetmi":
        strat_args = copy.deepcopy(args)
        strat_args['optimizer'] = "LazyGreedy"
        strat_args['wrapped_strategy_class'] = SMIAutoLabeler
        strat_args['smi_function'] = 'logdetmi'
        auto_labeling_strategy = PartitionStrategy(train_dataset, LabeledToUnlabeledDataset(lake_dataset), net, nclasses, strat_args, query_dataset=None)
    elif auto_labeling_name == "highest_confidence":
        strat_args = copy.deepcopy(args)
        auto_labeling_strategy = ConfidenceAutoLabeler(train_dataset, LabeledToUnlabeledDataset(lake_dataset), net, nclasses, strat_args)
    else:
        raise ValueError("auto_labeling_strategy should take one of ['fl1mi', 'fl2mi', 'gcmi', 'logdetmi', 'highest_confidence']")

    # Record the training transform and test transform for disabling purposes
    train_transform = full_dataset.transform
    test_transform = test_dataset.transform
    al_cnt = np.zeros(n_rounds)
    # Begin AL loop
    for rd in range(initial_round, n_rounds+1):

        print('-------------------------------------------------')
        print('Round', rd) 
        print('-------------------------------------------------')

        start_auto_time = time.time()
        full_dataset.transform = test_transform # Disable any augmentation while selecting points

        # Calculate selection budgets
        alpha_budget = math.ceil(alpha * budget)
        beta_budget = math.ceil((alpha + beta) * budget - alpha_budget)
        gamma_budget = budget - alpha_budget - beta_budget

        # ==============================
        # Auto + HIL-corrected Selection
        # ==============================

        # ---- Get Selected Idx WRPT Full Dataset ----

        # Get the auto-labeler selection budget, which will select points for auto-assigned labels 
        # and human-corrected suggested labels
        suggested_label_budget = alpha_budget + beta_budget

        # If the budget for doing per-class targeted selection is 0, simply assign empty lists for selected idx.
        # Otherwise, proceed with the selection.
        if suggested_label_budget == 0:
            auto_assigned_selected_idx = [[] for x in range(nclasses)]
            human_corrected_selected_idx = [[] for x in range(nclasses)]
        else:

            # Calculate idx with respect to lake set to label. Make sure to use current training set as the query set
            auto_labeling_strategy.update_queries(train_dataset)
            selected_idx = auto_labeling_strategy.select(suggested_label_budget)

            # Convert to full dataset indices
            for i, selected_idx_per_class in enumerate(selected_idx):
                selected_idx[i] = [(lake_indices[j], associated_gain) for (j, associated_gain) in selected_idx_per_class]

            # ---- Get Auto-Labeled, HIL-Corrected Portions ----

            # Now, we need to automatically assign labels to the top alpha points wrpt the scores attributed to each index selected.
            # For SMI strategies, this will be the marginal gain obtained during submodular maximization. For highest confidence, 
            # this will be the probability of the assigned class. We first determine the top alpha points.
        
            # Flatten selected_idx so that we can sort on the elements more easily.
            flattened_selected_idx = []
            for i, selection_row in enumerate(selected_idx):
                for j, selection_tuple in enumerate(selection_row):
                    flattened_selected_idx.append(selection_tuple)

            # Sort flattened array. We are sorting by the score, which is present in the last element of the tuple.
            sorted_flattened_selected_idx = sorted(flattened_selected_idx, key = lambda x: x[1], reverse = True)

            # Now, only get alpha_budget of these top points. Using these points, get the set of points that 
            # encompass those that have auto-assigned labels
            top_alpha_flattened_selected_idx = sorted_flattened_selected_idx[:alpha_budget]
            auto_assigned_idx_set = set([x[0] for x in top_alpha_flattened_selected_idx])

            # Partition selected_idx into auto_assigned_selected_idx, human_corrected_selected_idx
            auto_assigned_selected_idx = [[] for x in range(nclasses)]
            human_corrected_selected_idx = [[] for x in range(nclasses)]
            for i, selection_row in enumerate(selected_idx):
                for j, selection_tuple in enumerate(selection_row):
                    full_dataset_index = selection_tuple[0]
                    if full_dataset_index in auto_assigned_idx_set:
                        auto_assigned_selected_idx[i].append(selection_tuple)
                    else:
                        human_corrected_selected_idx[i].append(selection_tuple)

            # ---- Label These Points ----

            # Update the labels for auto-labeled points
            for sel_class, selected_idx_per_class in enumerate(auto_assigned_selected_idx):
                for (selected_index, _) in selected_idx_per_class:
                    auto_labeled_indices.append(selected_index)
                    assigned_labels[selected_index] = sel_class

            # Since we already have the ground-truth labels in this experiment, we do not 
            # need to take action for the human-corrected points. They will already have 
            # the correct label when assigned to the training dataset.

            # Update lake/train indices and lake_train_usage_list
            for selected_idx_per_class in selected_idx:
                for (selected_index, _) in selected_idx_per_class:
                    train_lake_usage_list[selected_index] = 1

            train_indices = [i for (i,x) in enumerate(train_lake_usage_list) if x == 1]
            lake_indices = [i for (i,x) in enumerate(train_lake_usage_list) if x == 0]

            # Now, update the train dataset and the lake dataset
            train_dataset = Subset(ReplaceLabelDataset(full_dataset, assigned_labels), train_indices)
            lake_dataset = Subset(full_dataset, lake_indices)

        start_al_time = time.time()

        # =======================
        # BEGIN PURE AL SELECTION
        # =======================
        # See if AL selection needs to be done. If not, assign blank lists as before.
        if gamma_budget == 0:
            selected_idx = []
        else:
            # ---- Get Selected Idx WRPT Full Dataset ----
            # First, be sure to update the active learning strategy with the new unlabeled dataset.
            active_learning_strategy.update_data(train_dataset, LabeledToUnlabeledDataset(lake_dataset))
            # Now, select the pure AL points.
            selected_idx = active_learning_strategy.select(gamma_budget)
            # selected_idx, unlike before, is flat with no scores. We simply map back to the full dataset.
            selected_idx = [lake_indices[j] for j in selected_idx]

            # ----- Label These Points ----
            # As before, we already have the ground-truth labels in this experiment; we do not 
            # need to take action for the human-corrected points. They will already have 
            # the correct label when assigned to the training dataset.
            # Update lake/train indices and lake_train_usage_list
            for selected_index in selected_idx:
                train_lake_usage_list[selected_index] = 1

            train_indices = [i for (i,x) in enumerate(train_lake_usage_list) if x == 1]
            lake_indices = [i for (i,x) in enumerate(train_lake_usage_list) if x == 0]
            al_cnt[rd-1] = len(selected_idx)
            # Now, update the train dataset and the lake dataset
            train_dataset = Subset(ReplaceLabelDataset(full_dataset, assigned_labels), train_indices)
            lake_dataset = Subset(full_dataset, lake_indices)

        end_al_time = time.time()

        # ===========================
        # RECORD SELECTION STATISTICS
        # ===========================

        # Knowing the disparity between the auto-assigned label and the true label is of interest.
        # We will keep track of this by building auto_assigned_selection_matrix and human_corrected_selection_matrix.
        # *_selection_matrix[i][j] corresponds to how many points of class j were 
        # selected using query set of class i. Ideally, we want high *_selection_matrix[i][i]
        # and low *_selection_matrix[i][j] for i != j.
        
        auto_assigned_selection_matrix = [[0 for y in range(nclasses)] for x in range(nclasses)]
        human_corrected_selection_matrix = [[0 for y in range(nclasses)] for x in range(nclasses)]

        # If there was any part of the budget assigned to the auto-assigned or human-corrected portions, 
        # then record the label counts.

        # Record label counts for auto-assigned points.
        if alpha_budget != 0:
            for i, selected_idx_per_class in enumerate(auto_assigned_selected_idx):
                auto_assigned_selection_matrix[i] = get_label_counts(Subset(full_dataset, [x[0] for x in selected_idx_per_class]), nclasses)

        # Record label counts for human-corrected points.
        if beta_budget != 0:
            for i, selected_idx_per_class in enumerate(human_corrected_selected_idx):
                human_corrected_selection_matrix[i] = get_label_counts(Subset(full_dataset, [x[0] for x in selected_idx_per_class]), nclasses) 

        # # Record label counts for human-corrected points.
        # if gamma_budget != 0:
        #     for i, selected_idx_per_class in enumerate(human_corrected_selected_idx):
        #         human_corrected_selection_matrix[i] = get_label_counts(Subset(full_dataset, [x[0] for x in selected_idx_per_class]), nclasses) 

        # Record selection times.
        auto_selection_time = start_al_time - start_auto_time
        al_selection_time = end_al_time - start_al_time

        # Add the recorded information to the experiment dictionary.
        exp_dict['auto_assigned_selected_idx'].append(auto_assigned_selected_idx)
        exp_dict['human_corrected_selected_idx'].append(human_corrected_selected_idx)
        exp_dict['active_learning_selected_idx'].append(selected_idx)
        exp_dict['auto_assigned_selection_matrices'].append(auto_assigned_selection_matrix)
        exp_dict['human_corrected_selection_matrices'].append(human_corrected_selection_matrix)
        exp_dict['auto_selection_times'].append(auto_selection_time)
        exp_dict['al_selection_times'].append(al_selection_time)
        exp_dict['al_cnt'] = al_cnt

        full_dataset.transform = train_transform # Re-enable any augmentation done during training

        # Update the data to both strategies one last time for this round.
        auto_labeling_strategy.update_data(train_dataset, LabeledToUnlabeledDataset(lake_dataset))
        active_learning_strategy.update_data(train_dataset, LabeledToUnlabeledDataset(lake_dataset))

        print("Auto Selection Time:", auto_selection_time)
        print("AL Selection Time:", al_selection_time)
        print('Number of training points -', len(train_dataset))

        # ==============
        # BEGIN TRAINING
        # ==============

        # Start training
        if balance_loss:
            train_dataset_auto_labeled_indices = []
            for (position_in_list, index) in enumerate(train_indices):
                if index in auto_labeled_indices:
                    train_dataset_auto_labeled_indices.append(position_in_list)
            dt.update_data(train_dataset, train_dataset_auto_labeled_indices)
        else:
            dt.update_data(train_dataset)
        start_train = time.time()
        clf = dt.train(None)
        end_train = time.time()

        # Record the new test accuracy, time spent in training, and corresponding set size
        test_acc = dt.get_acc_on_set(test_dataset)
        exp_dict['test_accuracies'].append(test_acc)
        exp_dict['set_sizes'].append(len(train_dataset))
        exp_dict['train_times'].append(end_train - start_train)

        # Update the model in the strategy
        auto_labeling_strategy.update_model(clf)
        active_learning_strategy.update_model(clf)
        print('Testing accuracy:', round(test_acc*100, 2), flush=True)

        # Create a checkpoint
        round_checkpoint = Checkpoint(exp_dict, train_lake_usage_list, assigned_labels, clf.state_dict(), experiment_name=experiment_name)
        round_checkpoint.save_checkpoint(checkpoint_directory)

    print('Training Completed')
    delete_checkpoints(checkpoint_directory, experiment_name)
    return exp_dict

"""### Experiment Fixture Creation"""

def get_tiny_imagenet(dataset_root_path):

    # Download and extract TinyImageNet if it isn't already.
    filepath = os.path.join(dataset_root_directory, "tiny-imagenet-200.zip")
    if not os.path.exists(filepath):
        download_command = F"wget -P {dataset_root_directory} http://cs231n.stanford.edu/tiny-imagenet-200.zip"
        os.system(download_command)

    dataset_path = os.path.join(dataset_root_directory, "tiny-imagenet-200")
    if not os.path.exists(dataset_path):
        with zipfile.ZipFile(filepath, 'r') as zip_ref:
            zip_ref.extractall(dataset_root_directory)

    # TinyImageNet has a test set, but it's labels are not available (following good practice).
    # Hence, we must evaluate on the validation set. We prepare the validation set according to 
    # https://towardsdatascience.com/pytorch-ignite-classifying-tiny-imagenet-with-efficientnet-e5b1768e5e8f
    # so that PyTorch's ImageFolder class can be used.
    validation_dir = os.path.join(dataset_path, 'val')

    # Open and read val annotations text file
    with open(os.path.join(validation_dir, 'val_annotations.txt'), 'r') as fp:
        data = fp.readlines()

    # Create image filename to class dictionary
    val_image_filename_to_class_dict = {}
    for line in data:
        words = line.split('\t')
        val_image_filename_to_class_dict[words[0]] = words[1]

    # Map each image into its own class folder
    old_val_img_dir = os.path.join(validation_dir, 'images')
    for img, folder in val_image_filename_to_class_dict.items():
        newpath = (os.path.join(validation_dir, folder, 'images'))
        if not os.path.exists(newpath):
            os.makedirs(newpath)
        if os.path.exists(os.path.join(old_val_img_dir, img)):
            os.rename(os.path.join(old_val_img_dir, img), os.path.join(newpath, img))
    if os.path.exists(old_val_img_dir):
        os.rmdir(old_val_img_dir)

def get_experiment_fixture(dataset_root_path, dataset_name, seed_set_size, model_name, model_base_path, init_model_train_args):

    # Load the dataset
    if dataset_name == "CIFAR10":

        train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])
        test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])

        full_train_dataset = datasets.CIFAR10(dataset_root_path, download=True, train=True, transform=train_transform)
        test_dataset = datasets.CIFAR10(dataset_root_path, download=True, train=False, transform=test_transform)

        nclasses = 10 # NUM CLASSES HERE

    elif dataset_name == "CIFAR100":

        train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])
        test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])

        full_train_dataset = datasets.CIFAR100(dataset_root_path, download=True, train=True, transform=train_transform)
        test_dataset = datasets.CIFAR100(dataset_root_path, download=True, train=False, transform=test_transform)

        nclasses = 100 # NUM CLASSES HERE

    elif dataset_name == "MNIST":

        image_dim=28
        train_transform = transforms.Compose([transforms.Resize((image_dim, image_dim)), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
        test_transform = transforms.Compose([transforms.Resize((image_dim, image_dim)), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])

        full_train_dataset = datasets.MNIST(dataset_root_path, download=True, train=True, transform=train_transform)
        test_dataset = datasets.MNIST(dataset_root_path, download=True, train=False, transform=test_transform)

        nclasses = 10 # NUM CLASSES HERE

    elif dataset_name == "TinyImageNet":

        get_tiny_imagenet(dataset_root_path)

        train_transform = transforms.Compose([transforms.RandomCrop(64, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])
        test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]) # ImageNet mean/std

        # Use val as test
        train_path = os.path.join(dataset_root_path, "tiny-imagenet-200", "train")
        test_path = os.path.join(dataset_root_path, "tiny-imagenet-200", "val")
        full_train_dataset = datasets.ImageFolder(train_path, transform=train_transform)
        test_dataset = datasets.ImageFolder(test_path, transform=test_transform)

        nclasses = 200 

    if model_name == "resnet18":
        model = ResNet18(num_classes=nclasses)
    elif model_name == "mnistnet":
        model = MnistNet()
    else:
        raise ValueError("Add model implementation")

    # Seed the rng used in dataset splits
    np.random.seed(42)

    # Retrieve the labels of the training set
    train_labels = get_labels(full_train_dataset)

    # Derive a list of indices that will represent the training set indices. The rest will represent the unlabeled set indices.
    per_class_size = seed_set_size // nclasses
    initial_train_idx = []
    for cls in range(nclasses):

        # Sample random points per class to form a balanced seed
        cls_idx = torch.where(train_labels==cls)[0]
        chosen_idx = np.random.choice(cls_idx, size=per_class_size, replace=False)
        initial_train_idx.extend(chosen_idx)

    # See if a model has already been trained for this fixture.
    model_name = F"{dataset_name}_{model_name}_{seed_set_size}"
    model_save_path = os.path.join(model_base_path, model_name)

    if os.path.isfile(model_save_path):
        print("Found Initial Model")
        state_dict = torch.load(model_save_path)
        model.load_state_dict(state_dict)
    else:
        print("Training Initial Model...")
        init_trainer = data_train(Subset(full_train_dataset, initial_train_idx), model, init_model_train_args)
        model = init_trainer.train(None)
        torch.save(model.state_dict(), model_save_path)

    return full_train_dataset, test_dataset, initial_train_idx, model, nclasses

"""## Pure Auto-Labeled

### FL1MI
"""

# alpha = 0.8
beta = 0.0
auto_labeling_strategy = "fl1mi"
# active_learning_strategy = "random"
args['num_partitions'] = 5 # FL1MI uses larger kernels; 5 partitions are needed
# balance_loss = False

auto_label_results_save_directory = os.path.join(base_save_directory, F"{alpha}_{beta}")
os.makedirs(auto_label_results_save_directory, exist_ok=True)

for (seed_set_size, budget, train_cap) in experiment_seed_budget_caps:
    train_dataset, test_dataset, init_train_idx, model, nclasses = get_experiment_fixture(dataset_root_directory, dataset_name, seed_set_size, model_name, model_directory, args)
    init_train_lake_usage_list = [1 if i in init_train_idx else 0 for i in range(len(train_dataset))]
    num_al_rounds = (train_cap - seed_set_size) // budget

    for run_count in range(per_exp_runs):
        if args['embedding_type'] == 'gradients':
            file_name_field = F"gradients_{args['gradType']}"
        elif args['embedding_type'] == 'features':
            file_name_field = F"features_{args['layer_name']}"

        # Get the results file name under which the results should be saved    
        experiment_results_file_name = F"{alpha}_{beta}_{balance_loss}_{dataset_name}_{model_name}_{seed_set_size}_{budget}_{train_cap}_{auto_labeling_strategy}_{active_learning_strategy}_{file_name_field}_{args['metric']}_{run_count}.json"
        experiment_results_path = os.path.join(auto_label_no_hil_save_directory, experiment_results_file_name)

        # Determine if this experiment needs to be run
        if not os.path.isfile(experiment_results_path):
            print("======================================")
            print(F"Running {experiment_results_file_name}")
            print("======================================")

            # There is no data for this run. Run this experiment again.
            results = al_train_loop(train_dataset, copy.deepcopy(init_train_lake_usage_list), test_dataset, copy.deepcopy(model), num_al_rounds, budget, args, nclasses, alpha, beta, auto_labeling_strategy, active_learning_strategy, checkpoint_directory, experiment_results_file_name, balance_loss=balance_loss)
            with open(experiment_results_path, "w") as write_file:
                json.dump(results, write_file)
        else:
            print("======================================")
            print(F"Results already obtained; skipping {experiment_results_file_name}")
            print("======================================")

"""### FL2MI"""

# alpha = 
beta = 0.0
auto_labeling_strategy = "fl2mi"
# active_learning_strategy = "random"
args['num_partitions'] = 1
# balance_loss = False

auto_label_results_save_directory = os.path.join(base_save_directory, F"{alpha}_{beta}")
os.makedirs(auto_label_results_save_directory, exist_ok=True)

for (seed_set_size, budget, train_cap) in experiment_seed_budget_caps:
    train_dataset, test_dataset, init_train_idx, model, nclasses = get_experiment_fixture(dataset_root_directory, dataset_name, seed_set_size, model_name, model_directory, args)
    init_train_lake_usage_list = [1 if i in init_train_idx else 0 for i in range(len(train_dataset))]
    num_al_rounds = (train_cap - seed_set_size) // budget

    for run_count in range(per_exp_runs):
        if args['embedding_type'] == 'gradients':
            file_name_field = F"gradients_{args['gradType']}"
        elif args['embedding_type'] == 'features':
            file_name_field = F"features_{args['layer_name']}"

        # Get the results file name under which the results should be saved    
        experiment_results_file_name = F"{alpha}_{beta}_{balance_loss}_{dataset_name}_{model_name}_{seed_set_size}_{budget}_{train_cap}_{auto_labeling_strategy}_{active_learning_strategy}_{file_name_field}_{args['metric']}_{run_count}.json"
        experiment_results_path = os.path.join(auto_label_no_hil_save_directory, experiment_results_file_name)

        # Determine if this experiment needs to be run
        if not os.path.isfile(experiment_results_path):
            print("======================================")
            print(F"Running {experiment_results_file_name}")
            print("======================================")

            # There is no data for this run. Run this experiment again.
            results = al_train_loop(train_dataset, copy.deepcopy(init_train_lake_usage_list), test_dataset, copy.deepcopy(model), num_al_rounds, budget, args, nclasses, alpha, beta, auto_labeling_strategy, active_learning_strategy, checkpoint_directory, experiment_results_file_name, balance_loss=balance_loss)
            with open(experiment_results_path, "w") as write_file:
                json.dump(results, write_file)
        else:
            print("======================================")
            print(F"Results already obtained; skipping {experiment_results_file_name}")
            print("======================================")

"""### GCMI"""
# alpha = 1.
beta = 0.0
auto_labeling_strategy = "gcmi"
# active_learning_strategy = "random"
args['num_partitions'] = 1
# balance_loss = False

auto_label_results_save_directory = os.path.join(base_save_directory, F"{alpha}_{beta}")
os.makedirs(auto_label_results_save_directory, exist_ok=True)

for (seed_set_size, budget, train_cap) in experiment_seed_budget_caps:
    train_dataset, test_dataset, init_train_idx, model, nclasses = get_experiment_fixture(dataset_root_directory, dataset_name, seed_set_size, model_name, model_directory, args)
    init_train_lake_usage_list = [1 if i in init_train_idx else 0 for i in range(len(train_dataset))]
    num_al_rounds = (train_cap - seed_set_size) // budget

    for run_count in range(per_exp_runs):
        if args['embedding_type'] == 'gradients':
            file_name_field = F"gradients_{args['gradType']}"
        elif args['embedding_type'] == 'features':
            file_name_field = F"features_{args['layer_name']}"

        # Get the results file name under which the results should be saved    
        experiment_results_file_name = F"{alpha}_{beta}_{balance_loss}_{dataset_name}_{model_name}_{seed_set_size}_{budget}_{train_cap}_{auto_labeling_strategy}_{active_learning_strategy}_{file_name_field}_{args['metric']}_{run_count}.json"
        experiment_results_path = os.path.join(auto_label_no_hil_save_directory, experiment_results_file_name)

        # Determine if this experiment needs to be run
        if not os.path.isfile(experiment_results_path):
            print("======================================")
            print(F"Running {experiment_results_file_name}")
            print("======================================")

            # There is no data for this run. Run this experiment again.
            results = al_train_loop(train_dataset, copy.deepcopy(init_train_lake_usage_list), test_dataset, copy.deepcopy(model), num_al_rounds, budget, args, nclasses, alpha, beta, auto_labeling_strategy, active_learning_strategy, checkpoint_directory, experiment_results_file_name, balance_loss=balance_loss)
            with open(experiment_results_path, "w") as write_file:
                json.dump(results, write_file)
        else:
            print("======================================")
            print(F"Results already obtained; skipping {experiment_results_file_name}")
            print("======================================")

"""### LogDetMI"""
beta = 0.0
auto_labeling_strategy = "logdetmi"
args['num_partitions'] = 5 # LogDetMI uses larger kernels; 5 partitions are needed
# balance_loss = False

auto_label_results_save_directory = os.path.join(base_save_directory, F"{alpha}_{beta}")
os.makedirs(auto_label_results_save_directory, exist_ok=True)

for (seed_set_size, budget, train_cap) in experiment_seed_budget_caps:
    train_dataset, test_dataset, init_train_idx, model, nclasses = get_experiment_fixture(dataset_root_directory, dataset_name, seed_set_size, model_name, model_directory, args)
    init_train_lake_usage_list = [1 if i in init_train_idx else 0 for i in range(len(train_dataset))]
    num_al_rounds = (train_cap - seed_set_size) // budget

    for run_count in range(per_exp_runs):
        if args['embedding_type'] == 'gradients':
            file_name_field = F"gradients_{args['gradType']}"
        elif args['embedding_type'] == 'features':
            file_name_field = F"features_{args['layer_name']}"

        # Get the results file name under which the results should be saved    
        experiment_results_file_name = F"{alpha}_{beta}_{balance_loss}_{dataset_name}_{model_name}_{seed_set_size}_{budget}_{train_cap}_{auto_labeling_strategy}_{active_learning_strategy}_{file_name_field}_{args['metric']}_{run_count}.json"
        experiment_results_path = os.path.join(auto_label_no_hil_save_directory, experiment_results_file_name)

        # Determine if this experiment needs to be run
        if not os.path.isfile(experiment_results_path):
            print("======================================")
            print(F"Running {experiment_results_file_name}")
            print("======================================")

            # There is no data for this run. Run this experiment again.
            results = al_train_loop(train_dataset, copy.deepcopy(init_train_lake_usage_list), test_dataset, copy.deepcopy(model), num_al_rounds, budget, args, nclasses, alpha, beta, auto_labeling_strategy, active_learning_strategy, checkpoint_directory, experiment_results_file_name, balance_loss=balance_loss)
            with open(experiment_results_path, "w") as write_file:
                json.dump(results, write_file)
        else:
            print("======================================")
            print(F"Results already obtained; skipping {experiment_results_file_name}")
            print("======================================")

"""### Highest Confidence"""

#alpha = 0
beta = 0.0
auto_labeling_strategy = "highest_confidence"
#active_learning_strategy = "random"
args['num_partitions'] = 1 # LogDetMI uses larger kernels; 5 partitions are needed
# balance_loss = False

auto_label_results_save_directory = os.path.join(base_save_directory, F"{alpha}_{beta}")
os.makedirs(auto_label_results_save_directory, exist_ok=True)

for (seed_set_size, budget, train_cap) in experiment_seed_budget_caps:
    train_dataset, test_dataset, init_train_idx, model, nclasses = get_experiment_fixture(dataset_root_directory, dataset_name, seed_set_size, model_name, model_directory, args)
    init_train_lake_usage_list = [1 if i in init_train_idx else 0 for i in range(len(train_dataset))]
    num_al_rounds = (train_cap - seed_set_size) // budget

    for run_count in range(per_exp_runs):
        if args['embedding_type'] == 'gradients':
            file_name_field = F"gradients_{args['gradType']}"
        elif args['embedding_type'] == 'features':
            file_name_field = F"features_{args['layer_name']}"

        # Get the results file name under which the results should be saved    
        experiment_results_file_name = F"{alpha}_{beta}_{balance_loss}_{dataset_name}_{model_name}_{seed_set_size}_{budget}_{train_cap}_{auto_labeling_strategy}_{active_learning_strategy}_{file_name_field}_{args['metric']}_{run_count}.json"
        experiment_results_path = os.path.join(auto_label_no_hil_save_directory, experiment_results_file_name)

        # Determine if this experiment needs to be run
        if not os.path.isfile(experiment_results_path):
            print("======================================")
            print(F"Running {experiment_results_file_name}")
            print("======================================")

            # There is no data for this run. Run this experiment again.
            results = al_train_loop(train_dataset, copy.deepcopy(init_train_lake_usage_list), test_dataset, copy.deepcopy(model), num_al_rounds, budget, args, nclasses, alpha, beta, auto_labeling_strategy, active_learning_strategy, checkpoint_directory, experiment_results_file_name, balance_loss=balance_loss)
            with open(experiment_results_path, "w") as write_file:
                json.dump(results, write_file)
        else:
            print("======================================")
            print(F"Results already obtained; skipping {experiment_results_file_name}")
            print("======================================")

"""## Pure Active Learning
"""
auto_label_al_save_directory = os.path.join(base_save_directory, 'AL')
alpha = 0.0
beta = 0.0
auto_labeling_strategy = "highest_confidence"
#active_learning_strategy = "random"
args['num_partitions'] = 1
# balance_loss = False

auto_label_results_save_directory = os.path.join(base_save_directory, F"{alpha}_{beta}")
os.makedirs(auto_label_results_save_directory, exist_ok=True)

for (seed_set_size, budget, train_cap) in experiment_seed_budget_caps:
    train_dataset, test_dataset, init_train_idx, model, nclasses = get_experiment_fixture(dataset_root_directory, dataset_name, seed_set_size, model_name, model_directory, args)
    init_train_lake_usage_list = [1 if i in init_train_idx else 0 for i in range(len(train_dataset))]
    num_al_rounds = (train_cap - seed_set_size) // budget

    for run_count in range(per_exp_runs):
        if args['embedding_type'] == 'gradients':
            file_name_field = F"gradients_{args['gradType']}"
        elif args['embedding_type'] == 'features':
            file_name_field = F"features_{args['layer_name']}"

        # Get the results file name under which the results should be saved    
        experiment_results_file_name = F"{alpha}_{beta}_{balance_loss}_{dataset_name}_{model_name}_{seed_set_size}_{budget}_{train_cap}_{auto_labeling_strategy}_{active_learning_strategy}_{file_name_field}_{args['metric']}_{run_count}.json"
        experiment_results_path = os.path.join(auto_label_no_hil_save_directory, experiment_results_file_name)

        # Determine if this experiment needs to be run
        if not os.path.isfile(experiment_results_path):
            print("======================================")
            print(F"Running {experiment_results_file_name}")
            print("======================================")

            # There is no data for this run. Run this experiment again.
            results = al_train_loop(train_dataset, copy.deepcopy(init_train_lake_usage_list), test_dataset, copy.deepcopy(model), num_al_rounds, budget, args, nclasses, alpha, beta, auto_labeling_strategy, active_learning_strategy, checkpoint_directory, experiment_results_file_name, balance_loss=balance_loss)
            with open(experiment_results_path, "w") as write_file:
                json.dump(results, write_file)
        else:
            print("======================================")
            print(F"Results already obtained; skipping {experiment_results_file_name}")
            print("======================================")

"""# PLOTTING

## Definitions

### Default Plot Styling
"""

matplotlib.rcParams['text.usetex'] = True
matplotlib.rcParams['axes.spines.right'] = False
matplotlib.rcParams['axes.spines.top'] = False
plt.rc('font', family='serif')
plt.rc('xtick', labelsize=18)
plt.rc('ytick', labelsize=18)
matplotlib.rc('text', usetex=True)
matplotlib.rcParams['text.latex.preamble']=[r"\usepackage{amsmath,amsfonts}"]
matplotlib.rcParams['text.latex.preamble']=[r"\usepackage{bm}"]
plt.rc('axes', linewidth=1)
plt.rc('font', weight='bold')
matplotlib.rcParams['text.latex.preamble'] = [r'\boldmath']

figdim = (12,6)
figdpi = 120

shade_alpha = 0.2
axis_label_font_size = 22

plot_title_font_size = 22

"""### Load Experiment Results"""

def get_experiment_results(auto_label_load_directory, dataset_name, model_name, alpha, beta, seed_set_size, budget, train_cap, per_exp_runs, args, balance_loss=False):

    #train_dataset, test_dataset, init_train_idx, model, nclasses = get_experiment_fixture(dataset_root_directory, dataset_name, seed_set_size, model_name, model_directory, args)
    #init_train_lake_usage_list = [1 if i in init_train_idx else 0 for i in range(len(train_dataset))]
    #num_al_rounds = (train_cap - seed_set_size) // budget
    exp_results_list = []

    for run_count in range(per_exp_runs):
        if args['embedding_type'] == 'gradients':
            file_name_field = F"gradients_{args['gradType']}"
        elif args['embedding_type'] == 'features':
            file_name_field = F"features_{args['layer_name']}"

        # Get the results file name    
        experiment_results_file_name = F"{alpha}_{beta}_{balance_loss}_{dataset_name}_{model_name}_{seed_set_size}_{budget}_{train_cap}_{args['smi_function']}_{args['al_strategy']}_{file_name_field}_{args['metric']}_{run_count}.json"
        experiment_results_path = os.path.join(auto_label_load_directory, experiment_results_file_name)

        # Determine if this experiment can be loaded
        if not os.path.isfile(experiment_results_path):
            print("======================================")
            print(F"Missing {experiment_results_path}")
            print("======================================")
        else:
            with open(experiment_results_path, "r") as json_file:
                exp_dict = json.load(json_file)
                exp_results_list.append(exp_dict)

    return exp_results_list

"""### Averages/STDs of Experiments

**Averages/STDs of Lists**
"""

def get_avg_std(list_of_lists):

    # Calculate average list
    avg_list = None
    for a_list in list_of_lists:
        if avg_list is None:
            avg_list = a_list
        else:
            avg_list = [(x + y) for (x,y) in zip(avg_list, a_list)]
    num_lists = len(list_of_lists)
    avg_list = [(x / num_lists) for x in avg_list]

    if num_lists == 1:
        std_list = [0.0 for x in range(len(avg_list))]
        return avg_list, std_list

    # Calculate sample standard dev. list
    std_list = None
    for a_list in list_of_lists:
        to_add_list = [(x - y)*(x - y) for (x,y) in zip(a_list, avg_list)]
        if std_list is None:
            std_list = to_add_list
        else:
            std_list = [(x + y) for (x,y) in zip(to_add_list, std_list)]
    std_list = [math.sqrt(x / (num_lists - 1)) for x in std_list]
    
    return avg_list, std_list

"""**Get Average/STD of Test Accuracies**"""

def get_avg_std_test_acc(list_of_exps):

    list_of_lists = []
    for exp in list_of_exps:
        list_of_lists.append(exp['test_accuracies'])
    return get_avg_std(list_of_lists)

"""**Get Average/STD of % Correctly Auto-Labeled Points**"""

def get_avg_std_correctly_labeled_points(list_of_exps):

    list_of_lists = []
    for exp in list_of_exps:
        selection_matrices = exp['selection_matrices']
        exp_correctly_labeled_points_list = []
        for selection_matrix in selection_matrices:
            nclasses = len(selection_matrix)
            correctly_labeled_points_frac = 0.
            for i in range(nclasses):
                correctly_labeled_points_frac += selection_matrix[i][i]
            correctly_labeled_points_frac /= sum([sum(selection_matrix_row) for selection_matrix_row in selection_matrix])
            exp_correctly_labeled_points_list.append(correctly_labeled_points_frac)
        list_of_lists.append(exp_correctly_labeled_points_list)
    return get_avg_std(list_of_lists)

"""**Get Average/STD of Cumulative Correctly Auto-Labeled Points**"""

def get_avg_std_total_corrections_needed(list_of_exps):

    list_of_lists = []
    for exp in list_of_exps:
        selection_matrices = exp['selection_matrices']
        exp_correctly_labeled_points_list = []
        working_sum = 0
        for selection_matrix in selection_matrices:
            nclasses = len(selection_matrix)
            correctly_labeled_points = 0
            for i in range(nclasses):
                correctly_labeled_points += selection_matrix[i][i]
            corrections_needed = exp['budget'] - correctly_labeled_points
            exp_correctly_labeled_points_list.append(working_sum + corrections_needed)
            working_sum += corrections_needed
        list_of_lists.append(exp_correctly_labeled_points_list)
    return get_avg_std(list_of_lists)

"""**Get Average/STD of # Incorrectly Labeled Points versus SMI Gain**"""

def get_avg_std_inc_label_smi(exp_save_directory, dataset_root_directory, dataset_name, model_name, model_directory, selection_mode, seed_set_size, budget, train_cap, per_exp_runs, args, round, budget_granularity=0.1):

    full_dataset, test_dataset, init_train_idx, model, nclasses = get_experiment_fixture(dataset_root_directory, dataset_name, seed_set_size, model_name, model_directory, args)
    list_of_exps = get_experiment_results(auto_label_hil_save_directory, dataset_root_directory, dataset_name, model_name, model_directory, selection_mode, seed_set_size, budget, train_cap, per_exp_runs, args)

    list_of_lists = []
    point_cutoffs = []
    for exp in list_of_exps:
        selection_matrix = exp['selected_idx'][round]
        correctness_gain_list = []
        for sel_class, selection_row in enumerate(selection_matrix):
            if len(selection_row) == 0:
                continue

            max_gain = max(selection_row, key=lambda x:x[1])[1]
            min_gain = min(selection_row, key=lambda x:x[1])[1]

            for sel_idx, gain in selection_row:
                _, true_label = full_dataset[sel_idx]
                normalized_gain = (gain - min_gain) / (max_gain - min_gain)
                correctness_gain_list.append((sel_class == true_label, normalized_gain))
        correctness_gain_list = sorted(correctness_gain_list, key=lambda x:x[1])
        budget = len(correctness_gain_list)
        num_budget_slices = int(1 / budget_granularity)
        point_cutoffs = [int(i * budget / num_budget_slices) for i in range(num_budget_slices + 1)]

        total_computed_gain = 0.
        incorrectly_labeled_total_list = []
        for point_cutoff in point_cutoffs:
            if point_cutoff == 0:
                gain_cutoff = -1
            else:
                gain_cutoff = correctness_gain_list[point_cutoff - 1][1]

            total_incorrect = 0

            for correct, norm_gain in correctness_gain_list:
                if norm_gain > gain_cutoff:
                    break
                if not correct:
                    total_incorrect += 1
            incorrectly_labeled_total_list.append(total_incorrect)
        list_of_lists.append(incorrectly_labeled_total_list)

    return point_cutoffs, get_avg_std(list_of_lists)

"""**Get Labeling Cost over Experiment**"""

def get_avg_labeling_costs(list_of_exps, cost_to_check_label, cost_to_assign_label):

    list_of_lists = []
    for exp in list_of_exps:
        exp_labeling_costs = [0]
        al_cnts = exp['al_cnt']
        working_sum = 0
        for al_cnt in al_cnts:
            round_labeling_cost = al_cnt * (cost_to_assign_label)
            working_sum += round_labeling_cost
            exp_labeling_costs.append(working_sum)
        list_of_lists.append(exp_labeling_costs)
    average_labeling_costs, std_labeling_costs = get_avg_std(list_of_lists)
    return average_labeling_costs


"""## Auto+AL Test Accuracy versus Labeling Cost"""
# Define a subset of the configurations to show here to avoid many, many lines
experiment_seed_budget_caps_to_show = experiment_seed_budget_caps

strategy_labels = {'fl1mi_a': 'Auto FL1MI',
                   'fl2mi_a': 'Auto FL2MI',
                   'gcmi_a': 'Auto GCMI',
                   'logdetmi_a': 'Auto LOGDETMI',
                   'confidence_a': 'Auto HC',
                   'al': active_learning_strategy.upper()}

strategy_colors = {'fl1mi_a': (1,0,0),
                   'fl2mi_a': (0,1,0),
                   'gcmi_a': (0,0,1),
                   'logdetmi_a': (1,1,0),
                   'confidence_a': (1,0,1),
                   'al': (0,0,0)}

shade_alpha = 0.25
cost_to_check_label = 1
cost_to_assign_label = 1

# Obtain the experiment results for our given configuration
for (seed_set_size, budget, train_cap) in experiment_seed_budget_caps_to_show:
    # Create a figure
    comparison_figdim = (22,7)
    comparison_fig, axes = plt.subplots(nrows=1, ncols=1, figsize=comparison_figdim, dpi=120, gridspec_kw = {'top':0.8})
    # Set titles and axis labels
    xlabel = r"\textbf{Labeling Cost}"
    ylabel = r"\textbf{Test Accuracy}"
    title = F"{seed_set_size} to {train_cap} using budget {budget} with Alpha={alpha}"
    comparison_fig.suptitle(title, fontsize=plot_title_font_size)
    axes[0].set_title(r"\textbf{Accuracy vs Labeling Cost}", fontsize=plot_title_font_size)
    axes[0].set_xlabel(xlabel, fontsize=axis_label_font_size)
    axes[0].set_ylabel(ylabel, fontsize=axis_label_font_size)
    lines_for_legend = []

    # Do auto fl1mi
    beta = 0.0
    selection_mode = "auto"
    args['smi_function'] = 'fl1mi'
    args['al_strategy'] = active_learning_strategy
    exp_results_list = get_experiment_results(auto_label_no_hil_save_directory, dataset_name, model_name, alpha, beta, seed_set_size, budget, train_cap, per_exp_runs, args, balance_loss)
    labeling_cost = get_avg_labeling_costs(exp_results_list, cost_to_check_label, cost_to_assign_label)
    average_acc, std = get_avg_std_test_acc(exp_results_list)
    lower_list = [(x-y) for  (x,y) in zip(average_acc,std)]
    upper_list = [(x+y) for (x,y) in zip(average_acc,std)]
    line = axes[0].plot(labeling_cost, average_acc, color=strategy_colors['fl1mi_a'], marker='o')[0]
    lines_for_legend.append(line)
    axes[0].fill_between(labeling_cost, lower_list, upper_list, alpha=shade_alpha, color=strategy_colors['fl1mi_a'])
    
    # Do auto fl2mi
    selection_mode = "auto"
    args['smi_function'] = 'fl2mi'
    args['al_strategy'] = active_learning_strategy
    exp_results_list = get_experiment_results(auto_label_no_hil_save_directory, dataset_name, model_name, alpha, beta, seed_set_size, budget, train_cap, per_exp_runs, args, balance_loss)
    labeling_cost = get_avg_labeling_costs(exp_results_list, cost_to_check_label, cost_to_assign_label)
    average_acc, std = get_avg_std_test_acc(exp_results_list)
    lower_list = [(x-y) for  (x,y) in zip(average_acc,std)]
    upper_list = [(x+y) for (x,y) in zip(average_acc,std)]
    line = axes[0].plot(labeling_cost, average_acc, color=strategy_colors['fl2mi_a'], marker='o')[0]
    lines_for_legend.append(line)
    axes[0].fill_between(labeling_cost, lower_list, upper_list, alpha=shade_alpha, color=strategy_colors['fl2mi_a'])
    
    # Do auto gcmi
    selection_mode = "auto"
    args['smi_function'] = 'gcmi'
    args['al_strategy'] = active_learning_strategy
    exp_results_list = get_experiment_results(auto_label_no_hil_save_directory, dataset_name, model_name, alpha, beta, seed_set_size, budget, train_cap, per_exp_runs, args, balance_loss)
    labeling_cost = get_avg_labeling_costs(exp_results_list, cost_to_check_label, cost_to_assign_label)
    average_acc, std = get_avg_std_test_acc(exp_results_list)
    lower_list = [(x-y) for  (x,y) in zip(average_acc,std)]
    upper_list = [(x+y) for (x,y) in zip(average_acc,std)]
    line = axes[0].plot(labeling_cost, average_acc, color=strategy_colors['gcmi_a'], marker='o')[0]
    lines_for_legend.append(line)
    axes[0].fill_between(labeling_cost, lower_list, upper_list, alpha=shade_alpha, color=strategy_colors['gcmi_a'])
    
    # Do auto logdetmi
    selection_mode = "auto"
    args['smi_function'] = 'logdetmi'
    args['al_strategy'] = active_learning_strategy
    exp_results_list = get_experiment_results(auto_label_no_hil_save_directory, dataset_name, model_name, alpha, beta, seed_set_size, budget, train_cap, per_exp_runs, args, balance_loss)
    labeling_cost = get_avg_labeling_costs(exp_results_list, cost_to_check_label, cost_to_assign_label)
    average_acc, std = get_avg_std_test_acc(exp_results_list)
    lower_list = [(x-y) for  (x,y) in zip(average_acc,std)]
    upper_list = [(x+y) for (x,y) in zip(average_acc,std)]
    line = axes[0].plot(labeling_cost, average_acc, color=strategy_colors['logdetmi_a'], marker='o')[0]
    lines_for_legend.append(line)
    axes[0].fill_between(labeling_cost, lower_list, upper_list, alpha=shade_alpha, color=strategy_colors['logdetmi_a'])
    
    # Do Auto highest confidence
    selection_mode = "auto"
    args['smi_function'] = 'highest_confidence'
    args['al_strategy'] = active_learning_strategy
    exp_results_list = get_experiment_results(auto_label_no_hil_save_directory, dataset_name, model_name, alpha, beta, seed_set_size, budget, train_cap, per_exp_runs, args, balance_loss)
    labeling_cost = get_avg_labeling_costs(exp_results_list, cost_to_check_label, cost_to_assign_label)
    average_acc, std = get_avg_std_test_acc(exp_results_list)
    lower_list = [(x-y) for  (x,y) in zip(average_acc,std)]
    upper_list = [(x+y) for (x,y) in zip(average_acc,std)]
    line = axes[0].plot(labeling_cost, average_acc, color=strategy_colors['confidence_a'], marker='o')[0]
    lines_for_legend.append(line)
    axes[0].fill_between(labeling_cost, lower_list, upper_list, alpha=shade_alpha, color=strategy_colors['confidence_a'])
    
    # Do AL
    selection_mode = "auto"
    args['smi_function'] = 'highest_confidence'
    args['al_strategy'] = active_learning_strategy
    exp_results_list = get_experiment_results(auto_label_no_hil_save_directory, dataset_name, model_name, alpha, beta, seed_set_size, budget, train_cap, per_exp_runs, args, balance_loss)
    # Random's labeling cost differs from the rest.
    labeling_cost = get_avg_labeling_costs(exp_results_list, cost_to_check_label, cost_to_assign_label)
    #labeling_cost = [exp_results_list[0]['budget'] * i * cost_to_assign_label for i in range(len(exp_results_list[0]['set_sizes']))]
    average_acc, std = get_avg_std_test_acc(exp_results_list)
    lower_list = [(x-y) for  (x,y) in zip(average_acc,std)]
    upper_list = [(x+y) for (x,y) in zip(average_acc,std)]
    line = axes[0].plot(labeling_cost, average_acc, color=strategy_colors['al'], marker='o')[0]
    lines_for_legend.append(line)
    axes[0].fill_between(labeling_cost, lower_list, upper_list, alpha=shade_alpha, color=strategy_colors['al'])
    label_list = [strategy_labels[key] for key in strategy_labels]
    label_perm = [0,1,2,3,4,5]
    label_list = [label_list[i] for i in label_perm]
    lines_for_legend = [lines_for_legend[i] for i in label_perm]
    comparison_fig.legend(lines_for_legend, label_list, loc="upper center", ncol=6, borderaxespad=3)
    # plt.show()
    jpg_file = F"{alpha}_{beta}_{balance_loss}_{dataset_name}_{model_name}_{seed_set_size}_{budget}_{train_cap}_{active_learning_strategy}_{file_name_field}_{args['metric']}.jpg"
    image_path = os.path.join(auto_label_no_hil_save_directory, jpg_file)
    plt.savefig(image_path)
